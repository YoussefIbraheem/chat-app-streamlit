---

```markdown
# ğŸ¦™ Chat App â€“ Streamlit + LLaMA 3

A lightweight, locally-run conversational AI app built using [Streamlit](https://streamlit.io/) and [Metaâ€™s LLaMA 3](https://ai.meta.com/llama/). This project marks my **first practical step into Generative AI** using open-source language models.

This app lets users interact with LLaMA 3 in a real-time chat interface, hosted locally â€” no external API calls or OpenAI dependencies required.

---

## ğŸ” Features

- ğŸ¦™ Runs **LLaMA 3 (2nd gen)** locally via `llama-cpp-python`
- âš¡ Efficient with support for quantized `.gguf` models
- âœ¨ Real-time chatbot interface using Streamlit
- ğŸ’¬ Maintains conversation state per session
- ğŸŒ™ Dark theme by default using Streamlit config

---

## ğŸš€ Demo

Coming soon â€” but you can run it locally in under a minute!

---

## ğŸ›  Installation

### 1. Clone the repo and set up your environment

```bash
git clone https://github.com/YoussefIbraheem/chat-app-streamlit.git
cd chat-app-streamlit

# Set up virtual environment
python -m venv .venv
source .venv/bin/activate       # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Download a LLaMA 3 GGUF model

Download a quantized `.gguf` model (e.g. `llama-3-8b-instruct.Q4_K_M.gguf`) from:

- [TheBloke on Hugging Face](https://huggingface.co/TheBloke)

Place the downloaded model file in:

```bash
models/llama-3-8b-instruct.Q4_K_M.gguf
```

> You can modify the model path in `app.py`.

### 3. Run the app

```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
chat-app-streamlit/
â”œâ”€â”€ app.py                   # Main Streamlit app
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ models/                  # Folder to store your .gguf model
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml          # Enables dark theme
```

---

## âš™ï¸ Model Notes

- Uses [`llama-cpp-python`](https://github.com/abetlen/llama-cpp-python)
- Supports **quantized GGUF format** for local LLaMA 3 inference
- No external API keys or cloud models required

---

## ğŸ’¡ Future Enhancements

- [ ] Token streaming for better UX
- [ ] RAG-style context injection (with PDFs or notes)
- [ ] Dockerization for deployment
- [ ] Prompt templates & personas

---

## ğŸ§‘â€ğŸ’» Author

**Youssef Ibraheem** â€“ Backend Developer diving into GenAI with open-source tooling  
[LinkedIn](https://www.linkedin.com/in/youssefibraheem/) Â· [GitHub](https://github.com/YoussefIbraheem)

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---
